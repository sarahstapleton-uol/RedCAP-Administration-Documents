Workflow in RedCAP

1.0 The diagram below describes the workflow of a typical project in RedCAP and will help you to get started: 

              ┌──────────────────────────────────┐
              │   REQUIREMENTS and DATA INPUTS   │
              │  - design (classic/longitudinal) │
              │  - governance and roles          │
              └──────────────────────────────────┘
                               │
                               ▼
              ┌──────────────────────────────────┐
              │   CREATE A PROJECT (DEV MODE)    │
              │  - basic settings and ID field   │
              │  - enable surveys / logging      │
              └──────────────────────────────────┘
                               │
                               ▼
      ┌────────────────────────┴───────────────────────────┐
      │                                                    │
      ▼                                                    ▼
┌───────────────────────────┐                 ┌───────────────────────────┐
│  CONFIGURE METADATA       │                 │  CONFIGURE SECURITY       │
│  - instruments and events │                 │  - user rights and roles  │
│  - branching and calcs    │                 │  - DAGs and export rights │
│  - data quality rules     │                 │  - API access control     │
└───────────────────────────┘                 └───────────────────────────┘
      │                                                    │
      └──────────────────────────┬─────────────────────────┘
                                 ▼
              ┌──────────────────────────────────┐
              │   TEST AND VALIDATE (DEV)        │
              │  - dummy records &andsurveys     │
              │  - check reports and exports     │
              │  - fix issues iteratively        │
              └──────────────────────────────────┘
                               │
                               ▼
              ┌──────────────────────────────────┐
              │   MOVE TO PRODUCTION             │
              │  - freeze structure              │
              │  - enable draft-mode changes     │
              └──────────────────────────────────┘
                               │
                               ▼
              ┌──────────────────────────────────┐
              │   ONGOING OPERATIONS             │
              │  - monitoring and logging        │
              │  - data quality and exports      │
              │  - controlled amendments         │
              │  - prepare for closeout          │
              └──────────────────────────────────┘

2.0 The Project Administrator's role is to oversee five domains that include the outcomes of a requirements analysis for the team, aligned to the RedCAP platform's capabilities.

1. To start the project, initialise the settings that govern user access (see ), and configure the structures that will organise the project. 
2. Ensure that secure access rights are aligned with roles, data access groups (DAGs) for locations, organizations or other corporate entities, logs and export/API access management (see: ). 
3. Technical teams will always advise using a test environment to catch the errors/glitches in a system before production and this step is strongly recommended. 

Other continuous quality development frameworks and project systems requirements templates also exist but this is an effective place sgo wshich 
starting point to begin thinking about the project as a series of phases and not isolated tasks. 

3.0 Workflow Design

    A. Requirements

    Some things to think about when designing your workflow are as follows:

    a.i Will the collection of data be simple to use? A significant problem on research projects is the distinction between recording the data accurately and recording that data in the correct location, which means a logical structure that is enforced by data quality controls must be enforced from the outset. This stipulation applies to data collected through patient-facing roles but also those in clinical laboratories including radiography, electrogram and recordings of the surgical use of technological tools. 

    a.ii Clarification will seem like an overtly simplistic recommendation but ensuring that the system you design meets the particularities of each member and participant group is fundamental to your success. Ensure that you understand the logical data flows in the system that you design and, in particular, how that data is going to look when you extract it from RedCAP; for instance, a longitudinal study requires time-stamp information and access to that information beyond the remit of audit trail could affect the ease with which analysis can be performed.

        ** Analyse classical and longitudinal requirements and how they are recorded in the data collection process and experiment with different data extraction methods to ensure that analysis is performed using optimal data structures. Particular attention should be granted to the events strcutures and visit records

        ** Repeating instruments and repeating events simplify the data entry approach and can make data analysis prone to error from duplicate entries, introduce repeating field names that are difficult to attribute and data analysis an uncertain process, with statistical tests that do not conform to the mathematical principles they use in expected results.

    b.1 User groups should know how to identify one another and who is responsible for what on the project, so that anomalies, human errors, duplicates, missing data and poorly constructed field names can be erradicated, as much as possible. According to the organizational culture, some ideas for formalising responsibilities are diagrams, people or data-flow driven, survey prefixes, data schemas and personnel briefings.

    B. Governance at the Foundations 

        * Are the team clear about how to gain ethical approvals in a timely manner? Do personnel (technical, external contractors) require expedited background checks before work begins? Does the study accommodate minutiae user groups, demographics, cross-border data protection regulations? 
        * Is data-de-identification process documented alongside a table of suitable permissions to both see and analyse this data? Are those personnel able to export the data directly? Where will the data be stored during analysis and post-export from the RedCAP platform? This is a simple step that can incuur the most negative penalties in terms of professional reputation, public enquiry and national scandal.
        * Consider keeping a checklist for your own records populated with responsibile owners and the dates those responsibilities were accepted.
    
    C. The development stage 

        c.i Enter the RedCAP project as the sysadmin and proceed to create a new project with minimal title and purpose fields complete. (It is always useful for new members of an institution to know that they are in the right place). 
        c.ii Select the most appropriate project type and wither enable or disable pre-existing research surveys. Use features such as time-zone, automated record ID, project language (s) and features surrounding auto-completion, automatic saving of data entries, logging (recommended)

    D. Configure metadata 
    
    The project team will be heavily invested in the structure and risk of the data collection platform but it responsibility rests with the sysadmin, so be assertive about getting the right answers and enforcing the correct usage. 
    
    d.i An Online Designer tool aims to simplify the process of using extensive data collections at the analysis stage. There is also the option to upload a Data Dictionary in a .csv file, which is precisely that; the definitive guide to what each data field name means in practice and its official name. This resource should not be guarded for the privileged use of data and systems professionals because it must also be the key resource in ensuring data is catalogued correctly. For example, imagine the use of the terms 'deliveries', 'babies', 'number of babies', and 'babies expected' in four different data sources/surveys/collections/projects. These inoccuous words could be interpreted to mean a record of a birth instance, a record of multiple babies born to one mother, and refer to early-infant mortality. In practice, the terminology was used to record date and time of birth, breathing status of the newborn, record 2, 3 or more infants born but never 1 baby from a single pregnancy, and a binary encoded field to confirm that the prospective mother was aware that she was pregnant. The data was all collected by a single project but entered and managed by different personnel, which, in the absence of a carefully controlled data dictionary or database schema, made the analysis extremely difficult to conduct.

    d.ii Data validation is a term used by managers and analysts to refer to the accuracy and suitability of the data recorded and the format selected for it. That is to say, the elements of a data field must adhere to the type of data that it is; a monetary value, weight or BMI are never character strings, and time should never be a numeric field. RedCAP has 'rules' entries to prevent the use of unsuitable entries, governed by numeric ranges of suitable values, cross-validation and consistency checks, and impossible values (D.O.B. 17-24-3025). Abeyance of these simple strictures reduces the risk of data corruption and invalidity. Specifically, RedCAP requires longitudinal studies to use Events and Arms for the structure of the project and the custom timeline is based on the use of designated events; date treatment commenced, presentation of symptoms after a pre-determined period of time, provision for domestic or community care. Its branching logic has field-testing and calulcation field testing provisions, which it is recommended to use prior to initialising a live environment. 

    d.iii Select the survey of interest for regular reporting and its specific fields to create a custom report. These reports update automatically (unless turned-off) so that instantaneous answers to regularly asked questions is feasible. Other options include checkboxes to indicate the entirety of a record is complete, or calendar reminders that a follow-up appointment is required.

    E. Secure Access

        There are many ways in which the platform can be made secure without compromising the access that essential personnel have to the data. It is the Systems Administrator's responsibility to manage this: 

        *   User accounts are manual or RedCAP authenticated/SSO mapping services
        *   User [Access] Rights are defined according to roles and responsibilities
        *   Data Access Groups (DAGs) are managed similairly.
            *   Confirm the conditions for intra-collaboration data visibility 
        *   API and standardised data exports are manageable through account management 
            *   It is recommended that only the minimum number of personnel can export the dataset
                ** Maintain a secure record of API keys and who has received one
    
    F. Production Environment

    Dummy records are very useful for testing the performance of the environment. Do not overlook the automated survey invitations if you are using them, and run the rules for data quality and exports. When you are confident in its performance and it meets the team's requirements, elect to move it into production. Any subsequent changes should be actioned to a draft model and they must be manually reviewed and approved before initialisation.

    G. Administrative Duties

    * Monitor the logs periodically for unexpected activity (deletion, edits at scale, peculiar login details)
    * Keep a Change Log ( a simple table works well) 
    * Produce data quality revviews/reports alongside the team 
    * Data export in R, Python, SQl, SAS and Tableau 
